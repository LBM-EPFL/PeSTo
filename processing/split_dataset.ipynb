{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters input\n",
    "pdb_clusters_url = \"https://cdn.rcsb.org/resources/sequence/clusters/bc-30.out\"\n",
    "training_exclusion_lists = [\n",
    "    \"data/lists/ppdb5_set.txt\",\n",
    "    \"data/lists/masif-site_test_set.txt\",\n",
    "    \"data/lists/skempi_v2.txt\",\n",
    "    \"data/lists/memcplxdb.txt\",\n",
    "    \"data/lists/excluded.txt\"\n",
    "]\n",
    "\n",
    "# parameters output\n",
    "np.random.seed(1337)\n",
    "train_ratio = 0.8\n",
    "output_dir = \"data/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37681 clusters for 567273 pdbs\n"
     ]
    }
   ],
   "source": [
    "# fetch sequence based clusters\n",
    "r = requests.get(pdb_clusters_url)\n",
    "assert r.status_code == 200\n",
    "raw_data = r.text\n",
    "\n",
    "# extract clusters\n",
    "pdb_clusters = []\n",
    "for line in raw_data.split('\\n'):\n",
    "    if len(line) > 0:\n",
    "        pdb_clusters.append([pdbid.strip() for pdbid in line.split(' ')])\n",
    "        \n",
    "# get all structure ids\n",
    "sids = np.concatenate(pdb_clusters)\n",
    "    \n",
    "# debug print\n",
    "print(f\"{len(pdb_clusters)} clusters for {sum(len(pdb_cluster) for pdb_cluster in pdb_clusters)} pdbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 567273/567273 [02:26<00:00, 3870.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281 subunits excluded (888 pdbids)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# exclude pdbs from training set based on pdbids (chain name may not be reliable)\n",
    "pdbids_excluded = []\n",
    "for fp in training_exclusion_lists:\n",
    "    with open(fp, 'r') as fs:\n",
    "        for line in fs:\n",
    "            if len(line) > 0:\n",
    "                pdbids_excluded.append(line.strip().split('_')[0])\n",
    "                \n",
    "# unique pdbids\n",
    "pdbids_excluded = np.unique(pdbids_excluded)\n",
    "                \n",
    "# exclude from pdbids\n",
    "training_excluded = []\n",
    "for sid in tqdm(sids):\n",
    "    for pdbid in pdbids_excluded:\n",
    "        if pdbid in sid.split('_')[0]:\n",
    "            training_excluded.append(sid)\n",
    "            \n",
    "# debug print\n",
    "print(f\"{len(training_excluded)} subunits excluded ({len(pdbids_excluded)} pdbids)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted: 36722 clusters / 479588 subunits / 0 subunits not located\n",
      "excluded: 959 clusters / 87685 subunits / 0 subunits not located\n"
     ]
    }
   ],
   "source": [
    "# define clusters mapping\n",
    "clusters_mapping = {}\n",
    "for k in range(len(pdb_clusters)):\n",
    "    for pdbid in pdb_clusters[k]:\n",
    "        clusters_mapping[pdbid] = k\n",
    "        \n",
    "# assign clusters\n",
    "sid_clusters_dict = {}\n",
    "not_located = []\n",
    "for sid in sids:\n",
    "    if sid in clusters_mapping:\n",
    "        cid = clusters_mapping[sid]\n",
    "        if cid in sid_clusters_dict:\n",
    "            sid_clusters_dict[cid].append(sid)\n",
    "        else:\n",
    "            sid_clusters_dict[cid] = [sid]\n",
    "    else:\n",
    "        not_located.append(sid)\n",
    "        \n",
    "# define clusters exclusion list\n",
    "clusters_exclusion_l = [clusters_mapping[sid] for sid in training_excluded if sid in clusters_mapping]\n",
    "not_located_exclusion = [sid for sid in training_excluded if sid not in clusters_mapping]\n",
    "\n",
    "# transform dict into list of list\n",
    "sid_clusters = [sid_clusters_dict[k] for k in sid_clusters_dict if k not in clusters_exclusion_l]\n",
    "sid_clusters_excluded = [sid_clusters_dict[k] for k in sid_clusters_dict if k in clusters_exclusion_l]\n",
    "\n",
    "# debug print\n",
    "print(f\"extracted: {len(sid_clusters)} clusters / {sum(len(sid_cluster) for sid_cluster in sid_clusters)} subunits / {len(not_located)} subunits not located\")\n",
    "print(f\"excluded: {len(sid_clusters_excluded)} clusters / {sum(len(sid_cluster) for sid_cluster in sid_clusters_excluded)} subunits / {len(not_located_exclusion)} subunits not located\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset: 29377 clusters / 381068 subunits\n",
      "testing dataset: 7345 clusters / 98520 subunits\n",
      "validation dataset: 959 clusters / 87685 subunits\n"
     ]
    }
   ],
   "source": [
    "# define and shuffle cluster indices\n",
    "N = len(sid_clusters)\n",
    "ids = np.arange(N)\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "# split training/testing subunits\n",
    "n = int(N*train_ratio)\n",
    "ids_train = ids[:n]\n",
    "ids_test = ids[n:]\n",
    "\n",
    "# define train subunits\n",
    "train_sids = []\n",
    "for i in ids_train:\n",
    "    train_sids.extend(sid_clusters[i])\n",
    "    \n",
    "# define test subunits\n",
    "test_sids = []\n",
    "for i in ids_test:\n",
    "    test_sids.extend(sid_clusters[i])\n",
    "\n",
    "# add excluded subunits to validation set\n",
    "valid_sids = []\n",
    "if len(sid_clusters_excluded) > 0:\n",
    "    for sid in np.concatenate(sid_clusters_excluded):\n",
    "        valid_sids.append(sid)\n",
    "\n",
    "# debug print\n",
    "print(f\"training dataset: {len(ids_train)} clusters / {len(train_sids)} subunits\")\n",
    "print(f\"testing dataset: {len(ids_test)} clusters / {len(test_sids)} subunits\")\n",
    "print(f\"validation dataset: {len(sid_clusters_excluded)} clusters / {len(valid_sids)} subunits\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# write train subunits ids\n",
    "with open(os.path.join(output_dir, \"subunits_train_set.txt\"), \"w\") as fs:\n",
    "    fs.write('\\n'.join(train_sids))\n",
    "\n",
    "# write test subunits ids\n",
    "with open(os.path.join(output_dir, \"subunits_test_set.txt\"), \"w\") as fs:\n",
    "    fs.write('\\n'.join(test_sids))\n",
    "    \n",
    "# write validation subunits ids\n",
    "with open(os.path.join(output_dir, \"subunits_validation_set.txt\"), \"w\") as fs:\n",
    "    fs.write('\\n'.join(valid_sids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesto",
   "language": "python",
   "name": "pesto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
